# ğŸ§  Insurance LLM Assistant

A smart assistant that uses a fine-tuned LLM to decide insurance claim approvals based on user queries and insurance policy clauses.

## ğŸ” Overview

This tool helps automate insurance claim decisions. Just enter a natural language query like:

> *"46-year-old male, knee surgery in Pune, 3-month-old insurance policy"*

The assistant will:
- Search relevant clauses using vector similarity
- Use a local LLM (LLaMA 3 via Ollama) to reason over them
- Return a structured decision including approval, amount, justification, and clause references.

---

## ğŸ—ï¸ Project Structure

INSURANCE_LLM-PROJECT/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ clauses.jsonl # Insurance policy clauses
â”‚ â”œâ”€â”€ inference.py # Main logic: query â†’ retrieval â†’ LLM â†’ output
â”‚ â”œâ”€â”€ ollama_client.py # Calls to LLaMA 3 via Ollama
â”‚ â”œâ”€â”€ streamlit_app.py # Frontend using Streamlit
â”‚ â””â”€â”€ init.py
â”‚
â”œâ”€â”€ embed/
â”‚ â””â”€â”€ build_index.py # Embeds clauses using MiniLM and builds FAISS index
â”‚
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # This file

yaml
Copy
Edit

---

## âš™ï¸ Setup Instructions

### 1. ğŸ Create & Activate Virtual Environment

```bash
conda create -n ins-env python=3.11 -y
conda activate ins-env
2. ğŸ’¾ Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
If using Ollama, also install it from https://ollama.com and run:

bash
Copy
Edit
ollama run llama3
3. ğŸ“¦ Build the Clause Index
Make sure clauses.jsonl exists, then:

bash
Copy
Edit
python embed/build_index.py
4. ğŸ§  Run the Assistant
bash
Copy
Edit
streamlit run app/streamlit_app.py
ğŸ§ª Example Output
Input:

hello guys I am ill

Output:

yaml
Copy
Edit
Approval: No
Amount: â‚¹0
Justification: Query is irrelevant or too vague.
Clause IDs Used: []
ğŸ› ï¸ Technologies Used
LLM: LLaMA 3 via Ollama

Embeddings: MiniLM (via sentence-transformers)

Vector Search: FAISS

UI: Streamlit

Backend Logic: Python

ğŸ“˜ Notes
Queries that are vague or irrelevant are automatically rejected.

You can fine-tune or swap the embedding or LLM model based on compute constraints.
